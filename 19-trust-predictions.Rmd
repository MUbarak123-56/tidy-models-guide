---
title: "When Should You Trust Your Predictions?"
author: "Mubarak Ganiyu"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Equivocal Results

```{r}
library(tidymodels)
tidymodels_prefer()

simulate_two_classes <-
  function (n, error = 0.1, eqn = quote(-1 - 2 * x - 0.2 * x^2 + 2 * y^2))  {
    # Slightly correlated predictors
    sigma <- matrix(c(1, 0.7, 0.7, 1), nrow = 2, ncol = 2)
    dat <- MASS::mvrnorm(n = n, mu = c(0, 0), Sigma = sigma)
    colnames(dat) <- c("x", "y")
    cls <- paste0("class_", 1:2)
    dat <-
      as_tibble(dat) %>%
      mutate(
        linear_pred = !!eqn,
        # Add some misclassification noise
        linear_pred = linear_pred + rnorm(n, sd = error),
        prob = binomial()$linkinv(linear_pred),
        class = ifelse(prob > runif(n), cls[1], cls[2]),
        class = factor(class, levels = cls)
      )
    dplyr::select(dat, x, y, class)
  }

set.seed(1901)
training_set <- simulate_two_classes(200)
testing_set  <- simulate_two_classes(50)

```


```{r}
two_class_mod <-
  logistic_reg() %>%
  set_engine("stan", seed = 1902) %>%
  fit(class ~ . + I(x^2)+ I(y^2), data = training_set)
print(two_class_mod, digits = 3)
```

```{r}
test_pred <- augment(two_class_mod, testing_set)
test_pred %>% head()
```

```{r}
library(probably)

lvls <- levels(training_set$class)

test_pred <-
  test_pred %>%
  mutate(.pred_with_eqz = make_two_class_pred(.pred_class_1, lvls, buffer = 0.15))

test_pred %>% count(.pred_with_eqz)
```

```{r}
test_pred %>% conf_mat(class, .pred_class)

test_pred %>% conf_mat(class, .pred_with_eqz)
```

```{r}
# A function to change the buffer then compute performance.
eq_zone_results <- function(buffer) {
  test_pred <-
    test_pred %>%
    mutate(.pred_with_eqz = make_two_class_pred(.pred_class_1, lvls, buffer = buffer))
  acc <- test_pred %>% accuracy(class, .pred_with_eqz)
  rep_rate <- reportable_rate(test_pred$.pred_with_eqz)
  tibble(accuracy = acc$.estimate, reportable = rep_rate, buffer = buffer)
}

# Evaluate a sequence of buffers and plot the results.
map_dfr(seq(0, .1, length.out = 40), eq_zone_results) %>%
  pivot_longer(c(-buffer), names_to = "statistic", values_to = "value") %>%
  ggplot(aes(x = buffer, y = value, lty = statistic)) +
  geom_step(size = 1.2, alpha = 0.8) +
  labs(y = NULL, lty = NULL)
```

```{r}
test_pred <-
  test_pred %>%
  bind_cols(
    predict(two_class_mod, testing_set, type = "pred_int", std_error = TRUE)
  )

test_pred
```

#### Determining model applicability

```{r}
## loads both `Chicago` dataset as well as `stations`
data(Chicago)

Chicago <- Chicago %>% select(ridership, date, one_of(stations))

n <- nrow(Chicago)

Chicago_train <- Chicago %>% slice(1:(n - 14))
Chicago_test  <- Chicago %>% slice((n - 13):n)
Chicago_test
```


```{r}
base_recipe <-
  recipe(ridership ~ ., data = Chicago_train) %>%
  # Create date features
  step_date(date) %>%
  step_holiday(date) %>%
  # Change date to be an id column instead of a predictor
  update_role(date, new_role = "id") %>%
  # Create dummy variables from factor columns
  step_dummy(all_nominal()) %>%
  # Remove any columns with a single unique value
  step_zv(all_predictors()) %>%
  step_normalize(!!!stations)

lm_spec <-
  linear_reg() %>%
  set_engine("lm")

lm_wflow <-
  workflow() %>%
  add_recipe(base_recipe) %>%
  add_model(lm_spec)

set.seed(1902)
lm_fit <- fit(lm_wflow, data = Chicago_train)
```

```{r}
# res_test <-
#   predict(lm_fit, Chicago_test) %>%
#   bind_cols(
#     predict(lm_fit, Chicago_test, type = "pred_int"),
#     Chicago_test
#   )
Chicago_test
```

```{r}
library(applicable)
pca_stat <- apd_pca(~ ., data = Chicago_train %>% select(one_of(stations)),
                    threshold = 0.99)
pca_stat
```

```{r}
autoplot(pca_stat, distance) + labs(x = "distance")
```

```{r}
score(pca_stat, Chicago_test) %>% select(starts_with("distance"))
```

