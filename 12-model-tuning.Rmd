---
title: "Model Tuning and the Dangers of Overfitting"
author: "Mubarak Ganiyu"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Loading data

```{r}
library(tidymodels)
tidymodels_prefer()
data(ames)
ames <- mutate(ames, Sale_Price = log10(Sale_Price))

set.seed(502)
ames_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test  <-  testing(ames_split)

ames_rec <-
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +
           Latitude + Longitude, data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>%
  step_other(Neighborhood, threshold = 0.01) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_") ) %>%
  step_ns(Latitude, Longitude, deg_free = 20)
train <- read.csv("train.csv")
test <- read.csv("test.csv")
train$Survived <- as.factor(train$Survived)
train_new <- train %>% 
  select(-PassengerId, -Name, -Cabin, -Ticket)
train_new$Sex <- as.factor(train_new$Sex)
train_new$Embarked <- as.factor(train_new$Embarked)
train_new
```

```{r}
library(tidymodels)
tidymodels_prefer()
train <- read.csv("train.csv")
test <- read.csv("test.csv")

llhood <- function(...) {
  logistic_reg() %>%
    set_engine("glm", ...) %>%
    fit(Survived ~ ., data = train_new) %>%
    glance() %>%
    select(logLik)
}

bind_rows(
  llhood(),
  llhood(family = binomial(link = "probit")),
  llhood(family = binomial(link = "cloglog"))
) %>%
  mutate(link = c("logit", "probit", "c-log-log"))  %>%
  arrange(desc(logLik))
```

```{r}
set.seed(1201)
rs <- vfold_cv(train_new, repeats = 10)

# Return the individual resampled performance estimates:
lloss <- function(...) {
  perf_meas <- metric_set(roc_auc, mn_log_loss)

  logistic_reg() %>%
    set_engine("glm", ...) %>%
    fit_resamples(Survived ~ Pclass + Age, rs, metrics = perf_meas) %>%
    collect_metrics(summarize = FALSE) %>%
    select(id, id2, .metric, .estimate)
}

resampled_res <-
  bind_rows(
    lloss()                                    %>% mutate(model = "logistic"),
    lloss(family = binomial(link = "probit"))  %>% mutate(model = "probit"),
    lloss(family = binomial(link = "cloglog")) %>% mutate(model = "c-log-log")
  ) %>%
  # Convert log-loss to log-likelihood:
  mutate(.estimate = ifelse(.metric == "mn_log_loss", -.estimate, .estimate)) %>%
  group_by(model, .metric) %>%
  summarize(
    mean = mean(.estimate, na.rm = TRUE),
    std_err = sd(.estimate, na.rm = TRUE) / sum(!is.na(.estimate)),
    .groups = "drop"
  )

resampled_res %>%
  filter(.metric == "mn_log_loss") %>%
  ggplot(aes(x = mean, y = model)) +
  geom_point() +
  geom_errorbar(aes(xmin = mean - 1.64 * std_err, xmax = mean + 1.64 * std_err),
                width = .1) +
  labs(y = NULL, x = "log-likelihood")
```

#### Tuning Parameters in tidymodels

```{r}
rand_forest(trees = 2000, min_n = 10) %>%                   # <- main arguments
  set_engine("ranger", regularization.factor = 0.5)         # <- engine-specific
```

```{r}
neural_net_spec <-
  mlp(hidden_units = tune()) %>%
  set_engine("keras")
```

```{r}
extract_parameter_set_dials(neural_net_spec)
```

```{r}
ames_rec <-
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +
           Latitude + Longitude, data = ames_train)  %>%
  step_log(Gr_Liv_Area, base = 10) %>%
  step_other(Neighborhood, threshold = tune()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_") ) %>%
  step_ns(Longitude, deg_free = tune("longitude df")) %>%
  step_ns(Latitude,  deg_free = tune("latitude df"))

recipes_param <- extract_parameter_set_dials(ames_rec)
recipes_param
```

```{r}
wflow_param <-
  workflow() %>%
  add_recipe(ames_rec) %>%
  add_model(neural_net_spec) %>%
  extract_parameter_set_dials()
wflow_param
```

```{r}
hidden_units()
threshold()
spline_degree()
```

```{r}
wflow_param %>% extract_parameter_dials("threshold")
```

```{r}
extract_parameter_set_dials(ames_rec) %>%
  update(threshold = threshold(c(0.8, 1.0)))
```

```{r}
rf_spec <-
  rand_forest(mtry = tune()) %>%
  set_engine("ranger", regularization.factor = tune("regularization"))

rf_param <- extract_parameter_set_dials(rf_spec)
rf_param
```
```{r}
rf_param %>%
  update(mtry = mtry(c(1, 70)))
```

```{r}
pca_rec <-
  recipe(Sale_Price ~ ., data = ames_train) %>%
  # Select the square-footage predictors and extract their PCA components:
  step_normalize(contains("SF")) %>%
  # Select the number of components needed to capture 95% of
  # the variance in the predictors.
  step_pca(contains("SF"), threshold = .95)

updated_param <-
  workflow() %>%
  add_model(rf_spec) %>%
  add_recipe(pca_rec) %>%
  extract_parameter_set_dials() %>%
  finalize(ames_train)

updated_param %>% extract_parameter_dials("mtry")
```
```{r}
rf_param
regularization_factor()
```

```{r}
penalty()

# correct method to have penalty values between 0.1 and 1.0
penalty(c(-1, 0)) %>% value_sample(1000) %>% summary()


# incorrect:
penalty(c(0.1, 1.0)) %>% value_sample(1000) %>% summary()
```

```{r}
penalty(trans = NULL, range = 10^c(-10, 0))
```

